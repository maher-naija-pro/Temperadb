name: Benchmark Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  # Allow manual triggering
  workflow_dispatch:
  # Run benchmarks on schedule (daily at 2 AM UTC)
  schedule:
    - cron: '0 2 * * *'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        go-version: ['1.20', '1.21', '1.22']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Go ${{ matrix.go-version }}
      uses: actions/setup-go@v4
      with:
        go-version: ${{ matrix.go-version }}
        
    - name: Init go.mod if missing
      run: |
        if [ ! -f "go.mod" ]; then
          echo "go.mod not found, initializing for current project..."
          go mod init timeseriesdb
        else
          echo "go.mod already exists."
        fi

    - name: Download dependencies
      run: go mod tidy
      
    - name: Verify dependencies
      run: go mod verify
      
    - name: Run all benchmarks
      run: make benchmark-all
      working-directory: .
      
    - name: Run benchmarks with profiling (CPU)
      run: |
        echo "Running CPU profiling benchmarks..."
        go test -bench=BenchmarkParseLargeDataset -cpuprofile=cpu_profile.prof -benchmem -timeout=10m ./test/
        echo "CPU profile generated: cpu_profile.prof"
      working-directory: .
      
    - name: Run benchmarks with profiling (Memory)
      run: |
        echo "Running memory profiling benchmarks..."
        go test -bench=BenchmarkMemoryUsage -memprofile=memory_profile.prof -benchmem -timeout=10m ./test/
        echo "Memory profile generated: memory_profile.prof"
      working-directory: .
      
    - name: Run individual benchmark categories
      run: |
        echo "=== Parser Benchmarks ==="
        make benchmark-parser
        
        echo "=== Storage Benchmarks ==="
        make benchmark-storage
        
        echo "=== HTTP Endpoint Benchmarks ==="
        make benchmark-http
        
        echo "=== End-to-End Benchmarks ==="
        make benchmark-e2e
        
        echo "=== Memory Usage Benchmarks ==="
        make benchmark-memory
      working-directory: .
      
    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-profiles-${{ matrix.go-version }}
        path: |
          *.prof
          coverage.out
        retention-days: 30
        
    - name: Clean up benchmark artifacts
      if: always()
      run: make benchmark-clean
      working-directory: .

  benchmark-performance:
    runs-on: ubuntu-latest
    needs: benchmark
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.22'
        
    - name: Init go.mod if missing
      run: |
        if [ ! -f "go.mod" ]; then
          echo "go.mod not found, initializing for current project..."
          go mod init timeseriesdb
        else
          echo "go.mod already exists."
        fi

    - name: Download dependencies
      run: go mod tidy
      
    - name: Run performance benchmarks with detailed output
      run: |
        echo "Running comprehensive performance benchmarks..."
        echo "================================================"
        
        # Run all benchmarks with detailed output
        go test -bench=. -benchmem -timeout=15m -v ./test/ | tee benchmark_results.txt
        
        echo "================================================"
        echo "Benchmark results saved to benchmark_results.txt"
      working-directory: .
      
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark_results.txt
        retention-days: 90
        
    - name: Comment on PR with benchmark summary
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          try {
            const results = fs.readFileSync('benchmark_results.txt', 'utf8');
            const summary = results.split('\n')
              .filter(line => line.includes('Benchmark'))
              .slice(0, 10) // Show first 10 benchmark results
              .join('\n');
              
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸš€ Benchmark Results Summary
              
              Here are the latest benchmark results for this PR:
              
              \`\`\`
              ${summary}
              \`\`\`
              
              Full results are available in the workflow artifacts.`
            });
          } catch (error) {
            console.log('Could not read benchmark results:', error.message);
          }
